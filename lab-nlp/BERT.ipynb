{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc91120c",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 3</a>\n",
    "## Fine-tuning BERT for the Product Review Problem - Classify Product Reviews as Positive or Not\n",
    "\n",
    "Let's fine-tune the BERT model to classify our product reviews. We will install a new library __transformers__ and get a pre-trained BERT model from it. We are following [this tutorial](https://huggingface.co/docs/transformers/training#train-in-native-pytorch) from the HuggingFace framework.\n",
    "\n",
    "We are using a light version of the original BERT implementation called __\"DistilBert\"__. You can checkout [their paper](https://arxiv.org/pdf/1910.01108.pdf) for more details. \n",
    "\n",
    "__Keep in mind that BERT and its variants use more resources than the other models we learned so far: recurrent neural networks, LSTMs etc. You may run out of memory sometimes. If that happens, you can restart the kernel (Kernel->Restart from the top menu), reduce the batch size and re-run the code.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64436f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f782fdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-20 18:10:08.527129: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-20 18:10:08.580661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-20 18:10:09.481215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926dfe2d",
   "metadata": {},
   "source": [
    "Let's read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af83f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TRAINING.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2a4ec",
   "metadata": {},
   "source": [
    "Let's print the dataset information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc624ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56000 entries, 0 to 55999\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   ID          56000 non-null  int64  \n",
      " 1   reviewText  55990 non-null  object \n",
      " 2   summary     55988 non-null  object \n",
      " 3   verified    56000 non-null  bool   \n",
      " 4   time        56000 non-null  int64  \n",
      " 5   log_votes   56000 non-null  float64\n",
      " 6   isPositive  56000 non-null  float64\n",
      "dtypes: bool(1), float64(2), int64(2), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654a4c80",
   "metadata": {},
   "source": [
    "We drop rows with text field missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4f993d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"reviewText\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2e21c",
   "metadata": {},
   "source": [
    "BERT requires powerful compute power. In this demo, we will only use the first 1,000 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "342b01ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf82541",
   "metadata": {},
   "source": [
    "We set the output type to int64 as it is required by this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd585b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"isPositive\"] = df[\"isPositive\"].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2d8b9",
   "metadata": {},
   "source": [
    "Let's keep 10% of the data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bbdd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This separates 10% of the entire dataset into validation dataset.\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"reviewText\"].tolist(),\n",
    "    df[\"isPositive\"].tolist(),\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    "    stratify = df[\"isPositive\"].tolist(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f7323",
   "metadata": {},
   "source": [
    "Let's get the special tokenizer for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5ff8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7827eb2e7cdd4dc0af9ce15997ed214d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c57712c57b64dd9a9341cd57305b799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25401d2dcecd435296300c7504d87737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce491342cea14c849367d93e93f6ca52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts,\n",
    "                            truncation=True,\n",
    "                            padding=True)\n",
    "val_encodings = tokenizer(val_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92881b",
   "metadata": {},
   "source": [
    "We prepare our data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332a8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]).to(device) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx]).to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "val_dataset = ReviewDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bbfae6",
   "metadata": {},
   "source": [
    "Let's call the model. This may print some warning messages. We are using it as intended, so don't worry about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42e0ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ea1baa27be46a9bc8beb49f8d7f661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                            num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e015dfe3",
   "metadata": {},
   "source": [
    "Let's start the fine-tuning process. This code may take __a long time__ to complete with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e98aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10174/675351116.py:20: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b28d54f15f541cba71017a8b7e39b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.6449. Val_loss 0.6209.     Val_accuracy 0.6354. Seconds 16.251.\n",
      "Epoch 1. Train_loss 0.6088. Val_loss 0.5848.     Val_accuracy 0.7083. Seconds 15.833.\n",
      "Epoch 2. Train_loss 0.5624. Val_loss 0.5490.     Val_accuracy 0.7917. Seconds 16.102.\n",
      "Epoch 3. Train_loss 0.5192. Val_loss 0.5114.     Val_accuracy 0.7604. Seconds 16.351.\n",
      "Epoch 4. Train_loss 0.4882. Val_loss 0.4759.     Val_accuracy 0.8125. Seconds 16.608.\n",
      "Epoch 5. Train_loss 0.4640. Val_loss 0.4762.     Val_accuracy 0.7396. Seconds 16.499.\n",
      "Epoch 6. Train_loss 0.4262. Val_loss 0.4507.     Val_accuracy 0.8125. Seconds 16.262.\n",
      "Epoch 7. Train_loss 0.4342. Val_loss 0.4221.     Val_accuracy 0.8229. Seconds 16.190.\n",
      "Epoch 8. Train_loss 0.4104. Val_loss 0.4194.     Val_accuracy 0.8333. Seconds 16.172.\n",
      "Epoch 9. Train_loss 0.4043. Val_loss 0.4843.     Val_accuracy 0.7917. Seconds 16.183.\n"
     ]
    }
   ],
   "source": [
    "# Freeze the encoder weights until the classfier\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "learning_rate=0.01\n",
    "\n",
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, drop_last=True)\n",
    "eval_dataloader = DataLoader(val_dataset, batch_size=8, drop_last=True)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "model=model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    val_loss = 0\n",
    "    # Training loop starts\n",
    "    model.train() # put the model in training mode\n",
    "    for batch in train_dataloader:\n",
    "        # below: ** allows us to pass multiple arguments to model()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    # Validation loop starts\n",
    "    model.eval() # put the model in prediction mode\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            # below:  ** allows us to pass multiple arguments to model()\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        val_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_dataloader)\n",
    "    val_loss = val_loss / len(eval_dataloader)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch}. Train_loss {training_loss:.4f}. Val_loss {val_loss:.4f}. \\\n",
    "    Val_accuracy {metric.compute()['accuracy']:.4f}. Seconds {end-start:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f7ce78",
   "metadata": {},
   "source": [
    "### Looking at what's going on\n",
    "\n",
    "The fine-tuned BERT is able to correctly classify the sentiment of all records in the validation set. Let's print some of the data and what's happening with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96bc7d33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "[101, 1045, 4149, 2023, 2138, 6881, 3769, 1011, 2039, 21628, 2015, 2020, 4760, 2039, 2006, 2026, 12191, 1012, 2023, 10770, 3036, 4031, 2134, 1005, 1056, 2131, 9436, 1997, 2068, 1010, 2061, 1045, 2001, 9364, 1010, 2021, 2009, 2052, 3796, 1996, 4180, 1012, 2061, 2009, 4066, 1997, 2499, 1010, 2021, 1045, 4299, 2009, 2071, 4550, 2039, 2026, 3274, 2062, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "I bought this because weird pop-up tabs were showing up on my laptop. This Norton security product didn't get rid of them, so I was disappointed, but it would block the content.  So it sort of worked, but I wish it could clean up my computer more.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "print(len(val_dataset.encodings[\"input_ids\"][k]))\n",
    "print(val_dataset.encodings[\"input_ids\"][k])\n",
    "print(val_texts[k])\n",
    "print(val_labels[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b996a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "[101, 1045, 2031, 2109, 2119, 22432, 7959, 2063, 1998, 10770, 1998, 2044, 2383, 2109, 2023, 4007, 2005, 2058, 1037, 2095, 2085, 1045, 2079, 2025, 2933, 2000, 2689, 1012, 2009, 2515, 2025, 4030, 2091, 2026, 3274, 1012, 1045, 2224, 2009, 2006, 2026, 7473, 1998, 14960, 1012, 2009, 2038, 7420, 2033, 1997, 4022, 4795, 4773, 4573, 2008, 1996, 2060, 2048, 2106, 2025, 1998, 2009, 2003, 16286, 21125, 1012, 6581, 4007, 2005, 4274, 3036, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "I have used both McAfee and Norton and after having used this software for over a year now I do not plan to change. It does not slow down my computer. I use it on my PC and notebook. It has warned me of potential dangerous web sites that the other two did not and it is reasonably priced. Excellent software for internet security.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "k = 24\n",
    "print(len(val_dataset.encodings[\"input_ids\"][k]))\n",
    "print(val_dataset.encodings[\"input_ids\"][k])\n",
    "print(val_texts[k])\n",
    "print(val_labels[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa481a1e",
   "metadata": {},
   "source": [
    "Let's observe in more detail how sentences are tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "195dd23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have used both McAfee and Norton and after having used this software for over a year now I do not plan to change. It does not slow down my computer. I use it on my PC and notebook. It has warned me of potential dangerous web sites that the other two did not and it is reasonably priced. Excellent software for internet security.\n",
      "{'input_ids': [101, 1045, 2031, 2109, 2119, 22432, 7959, 2063, 1998, 10770, 1998, 2044, 2383, 2109, 2023, 4007, 2005, 2058, 1037, 2095, 2085, 1045, 2079, 2025, 2933, 2000, 2689, 1012, 2009, 2515, 2025, 4030, 2091, 2026, 3274, 1012, 1045, 2224, 2009, 2006, 2026, 7473, 1998, 14960, 1012, 2009, 2038, 7420, 2033, 1997, 4022, 4795, 4773, 4573, 2008, 1996, 2060, 2048, 2106, 2025, 1998, 2009, 2003, 16286, 21125, 1012, 6581, 4007, 2005, 4274, 3036, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "st = val_texts[24]\n",
    "print(st)\n",
    "tok = tokenizer(st, truncation=True, padding=True)\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c78bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The mapped vocabulary is stored in tokenizer.vocab\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d8ae225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'i', 'have', 'used', 'both', 'mca', '##fe', '##e', 'and', 'norton', 'and', 'after', 'having', 'used', 'this', 'software', 'for', 'over', 'a', 'year', 'now', 'i', 'do', 'not', 'plan', 'to', 'change', '.', 'it', 'does', 'not', 'slow', 'down', 'my', 'computer', '.', 'i', 'use', 'it', 'on', 'my', 'pc', 'and', 'notebook', '.', 'it', 'has', 'warned', 'me', 'of', 'potential', 'dangerous', 'web', 'sites', 'that', 'the', 'other', 'two', 'did', 'not', 'and', 'it', 'is', 'reasonably', 'priced', '.', 'excellent', 'software', 'for', 'internet', 'security', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Methods convert_ids_to_tokens and convert_tokens_to_ids allow to see how sentences are tokenized\n",
    "print(tokenizer.convert_ids_to_tokens(tok['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89346761",
   "metadata": {},
   "source": [
    "# Getting predictions on the test data and saving results\n",
    "* Read the test data\n",
    "* Pass the data into your pipeline and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd36b78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33276</td>\n",
       "      <td>I've been using greeting card software for wel...</td>\n",
       "      <td>Absolutely awful.</td>\n",
       "      <td>False</td>\n",
       "      <td>1300233600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20859</td>\n",
       "      <td>This version worked well for me, have upgraded...</td>\n",
       "      <td>Good for virtual machine on a mac</td>\n",
       "      <td>True</td>\n",
       "      <td>1448755200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63500</td>\n",
       "      <td>Great!</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1456963200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4950</td>\n",
       "      <td>I can assure you that any five star review was...</td>\n",
       "      <td>SCAM</td>\n",
       "      <td>False</td>\n",
       "      <td>1400803200</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26509</td>\n",
       "      <td>Overall the product really seems the same but ...</td>\n",
       "      <td>Has potential but many glitches and really the...</td>\n",
       "      <td>False</td>\n",
       "      <td>1419206400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                         reviewText  \\\n",
       "0  33276  I've been using greeting card software for wel...   \n",
       "1  20859  This version worked well for me, have upgraded...   \n",
       "2  63500                                             Great!   \n",
       "3   4950  I can assure you that any five star review was...   \n",
       "4  26509  Overall the product really seems the same but ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Absolutely awful.     False  1300233600   \n",
       "1                  Good for virtual machine on a mac      True  1448755200   \n",
       "2                                         Five Stars      True  1456963200   \n",
       "3                                               SCAM     False  1400803200   \n",
       "4  Has potential but many glitches and really the...     False  1419206400   \n",
       "\n",
       "   log_votes  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.000000  \n",
       "3   2.197225  \n",
       "4   0.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the test data (It doesn't have the human_tag label, we are trying to predict that :D )\n",
    "df_test = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TEST.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fabc8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID            0\n",
       "reviewText    1\n",
       "summary       2\n",
       "verified      0\n",
       "time          0\n",
       "log_votes     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cb479bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"reviewText\"] = df_test[\"reviewText\"].fillna(value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5869a890",
   "metadata": {},
   "source": [
    "Below, we only consider 100 test datapoints to keep this short. Use the whole test dataset if you want to apply this on your final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a35daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = df_test[\"reviewText\"].tolist()[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f19e9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encodings = tokenizer(test_texts,\n",
    "                          truncation=True,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25e8ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ReviewDataset(test_encodings, [0]*len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4012e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=4)\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    test_predictions.extend(predictions.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f674a041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "[101, 1045, 1005, 2310, 2042, 2478, 14806, 4003, 4007, 2005, 2092, 2058, 2184, 2086, 1998, 1045, 1005, 2310, 2196, 2272, 2408, 1037, 4013, 22864, 2061, 3697, 1010, 2065, 2025, 5263, 1010, 2000, 2224, 1012, 2000, 2707, 1010, 1996, 8128, 2681, 2307, 8198, 2073, 2592, 2323, 2022, 1012, 1996, 3784, 2490, 2008, 1045, 7303, 2393, 2013, 2196, 5838, 1012, 2017, 2064, 2069, 2147, 2006, 2028, 3931, 2012, 1037, 2051, 2029, 2965, 2043, 12697, 1996, 2503, 3659, 2006, 1037, 4003, 1010, 2017, 2064, 1005, 1056, 2156, 2119, 5530, 7453, 1012, 1045, 2145, 4033, 1005, 1056, 2042, 2583, 2000, 7523, 2129, 2000, 2079, 2048, 1011, 11536, 8021, 1998, 2045, 1005, 1055, 2498, 1999, 1996, 8128, 2000, 2393, 2033, 2041, 1012, 9343, 2023, 4031, 2001, 1037, 4121, 6707, 1012, 2022, 8059, 1012, 5125, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "I've been using greeting card software for well over 10 years and I've never come across a progam so difficult, if not impossible, to use. To start, the instructions leave great holes where information should be. The online support that I requested help from never responded. You can only work on one page at a time which means when designing the inside spread on a card, you can't see both pages simultaneously. I still haven't been able to discover how to do two-sided printing and there's nothing in the instructions to help me out. Buying this product was a huge mistake. Beware.\n",
      "\n",
      "Victor\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "print(len(test_dataset.encodings[\"input_ids\"][k]))\n",
    "print(test_dataset.encodings[\"input_ids\"][k])\n",
    "print(test_texts[k])\n",
    "#check whether the prediction is good enough\n",
    "print(test_predictions[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd1c67",
   "metadata": {},
   "source": [
    "Again, we used only 100 test datapoints below. Use the full test set for your final project if you are interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71d1cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = df_test[\"ID\"][0:100]\n",
    "result_df[\"isPositive\"] = test_predictions\n",
    "\n",
    "result_df.to_csv(\"result_day3_bert.csv\", encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
